{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas\n",
    "import re\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the raw JSON file\n",
    "json_file = open(\"data/Takeout/YouTube and YouTube Music/history/watch-history.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load as dict\n",
    "data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ads(data):\n",
    "    '''\n",
    "    Filters ads from watched videos.\n",
    "\n",
    "    Args:\n",
    "        data: The list of watched videos.\n",
    " \n",
    "    Returns:\n",
    "        list[dict]: A list of watched videos without ads included.\n",
    "    '''\n",
    "\n",
    "    no_ads = []\n",
    "    print(f\"before removing ads: {len(data)}\")\n",
    "    for vid in data:\n",
    "        if 'details' in vid and 'name' in vid['details'][0] and \"From Google Ads\" in vid['details'][0]['name']:\n",
    "            continue\n",
    "        no_ads.append(vid)\n",
    "\n",
    "    print(f\"after removing ads: {len(no_ads)}\")\n",
    "\n",
    "    return no_ads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before removing ads: 45000\n",
      "after removing ads: 31332\n"
     ]
    }
   ],
   "source": [
    "data = remove_ads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UTC_convert(date_time_str, start_analysis, end_analysis, start_timeframe_1, end_timeframe_1, start_timeframe_2, end_timeframe_2):\n",
    "    '''\n",
    "    Updates dates and times to match the correct timezones. If a timestamp lands between the specified timeframes, it \n",
    "    is updated according to Malaysian time (+8 hours). Otherwise, it is updated according to Melbourne time (+11 hours).\n",
    "    Filters out timestamps that are not in 2023.\n",
    "\n",
    "    Args:\n",
    "        date_time_str: The string representation of the timestamp to update.\n",
    "        start_analysis: The start of the entire period to analyse.\n",
    "        end_analysis: The end of the entire period to analyse.\n",
    "        start_timeframe_1: The start of timeframe 1.\n",
    "        end_timeframe_1: The end of timeframe 1.\n",
    "        start_timeframe_2: The start of timeframe 2.\n",
    "        end_timeframe_2: The end of timeframe 2.\n",
    "    \n",
    "    Returns:\n",
    "        str: The string representation of the updated timestamp.\n",
    "        or \n",
    "        None: If the timestamp is not within 2023.\n",
    "    '''\n",
    "    \n",
    "    # Convert the date-time string to a datetime object\n",
    "    date_time = datetime.strptime(date_time_str, \"%Y-%m-%dT%H:%M\")\n",
    "\n",
    "    # Update `date_time`s by different amounts depending on the timeframe \n",
    "    # curr_time = date_time.datetime()\n",
    "    updated_date_time = None\n",
    "    if not start_analysis <= date_time <= end_analysis:\n",
    "        # Return None if timestamp is out of the analysis period\n",
    "        return updated_date_time\n",
    "    elif start_timeframe_1 <= date_time <= end_timeframe_1 or start_timeframe_2 <= date_time <= end_timeframe_2:\n",
    "        updated_date_time = date_time + timedelta(hours=8)\n",
    "    else:\n",
    "        updated_date_time = date_time + timedelta(hours=11)\n",
    "\n",
    "    # Convert the updated datetime object back to a string\n",
    "    updated_date_time_str = updated_date_time.strftime(\"%Y-%m-%dT%H:%M\")\n",
    "\n",
    "    return updated_date_time_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_vids(data):\n",
    "    '''\n",
    "    Preprocesses watched videos by:\n",
    "    1. Removing videos that were watched outside of 2023, incrementing dates/times to match the correct timezones depending \n",
    "        on my travel history, and creating two separate fields for \"date\" and \"time\" to support downstream analysis\n",
    "    2. Removing \"Watched\" which is sometimes prepended to video titles\n",
    "    3. Making a new un-nested field \"channel\" for the channel name, which is \"unknown\" if this is unknown \n",
    "\n",
    "    Args:\n",
    "        updated_data: The list of watched videos.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: The list of preprocessed videos.\n",
    "    '''\n",
    "\n",
    "    # Define timeframes as datetime objects.\n",
    "    # `start_analysis` and `end_analysis` indicate the desired period to analyse.\n",
    "    start_analysis = datetime.strptime(\"2022-12-31T16:00\", \"%Y-%m-%dT%H:%M\")\n",
    "    end_analysis = datetime.strptime(\"2024-2-1T11:00\", \"%Y-%m-%dT%H:%M\")\n",
    "    # `start_timeframe`s and `end_timeframe`s indicate the timeframes when I was overseas.\n",
    "    start_timeframe_1 = datetime.strptime(\"2023-06-24T20:30\", \"%Y-%m-%dT%H:%M\")\n",
    "    end_timeframe_1 = datetime.strptime(\"2023-06-30T10:50\", \"%Y-%m-%dT%H:%M\")\n",
    "    start_timeframe_2 = datetime.strptime(\"2022-12-31T13:00\", \"%Y-%m-%dT%H:%M\")\n",
    "    end_timeframe_2 = datetime.strptime(\"2023-02-24T14:20\", \"%Y-%m-%dT%H:%M\")\n",
    "\n",
    "    within_2023 = []\n",
    "    for vid in data:\n",
    "        # Update datetime to match timezones, and split the datetime into date and time fields\n",
    "        vid['time'] = ':'.join(vid['time'].split(':')[:-1])\n",
    "        updated_date_time = UTC_convert(vid['time'], start_analysis, end_analysis, start_timeframe_1, end_timeframe_1, start_timeframe_2, end_timeframe_2)\n",
    "        if updated_date_time is None:\n",
    "            continue\n",
    "        updated_date_time = updated_date_time.split('T')\n",
    "        vid['date'] = updated_date_time[0]\n",
    "        vid['time'] = updated_date_time[1]\n",
    "        within_2023.append(vid)\n",
    "\n",
    "\n",
    "    for vid in within_2023:\n",
    "\n",
    "        vid['title'] = re.sub(r'Watched\\s|,|\\r', '', vid['title'])\n",
    "\n",
    "        if 'subtitles' in vid and 'name' in vid['subtitles'][0]:\n",
    "            vid['channel'] = re.sub(r',|\\r', '', vid['subtitles'][0]['name'])\n",
    "        else:\n",
    "            # Channel name is unknown\n",
    "            vid['channel'] = 'unknown'\n",
    "    \n",
    "    return within_2023\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_vids(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'header': 'YouTube',\n",
       " 'title': 'A 3am Classical Playlist',\n",
       " 'titleUrl': 'https://www.youtube.com/watch?v=WYmwP7F8-X8',\n",
       " 'subtitles': [{'name': 'Classical Radio',\n",
       "   'url': 'https://www.youtube.com/channel/UC3DFPlJRkK9VWckgfBCtwJw'}],\n",
       " 'time': '03:57',\n",
       " 'products': ['YouTube'],\n",
       " 'activityControls': ['YouTube watch history'],\n",
       " 'date': '2024-01-31',\n",
       " 'channel': 'Classical Radio'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that data was preprocessed as expected\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_csv(data, fields):\n",
    "    '''\n",
    "    Saves JSON object to CSV file, only writing the specified fields within each item. \n",
    "\n",
    "    Args:\n",
    "        data: The list of watched videos.\n",
    "        fields: The list of fields to write\n",
    "    '''\n",
    "    \n",
    "    with open('data/watch_history.csv', 'w', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "\n",
    "        # Write the header\n",
    "        csv_writer.writerow(fields)\n",
    "\n",
    "        # Iterate through each JSON object in the list\n",
    "        for obj in data:\n",
    "            # Extract values for the specified fields from each JSON object\n",
    "            row_values = [obj.get(field, '') for field in fields]\n",
    "\n",
    "            # Write the row to the CSV file\n",
    "            csv_writer.writerow(row_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_to_csv(data, [\"title\", \"date\", \"time\", \"channel\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
